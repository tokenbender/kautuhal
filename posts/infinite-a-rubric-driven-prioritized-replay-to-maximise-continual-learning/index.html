<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>infinite - a rubric driven prioritized replay to maximise continual learning - tokenbender</title>
    <meta name="description" content="a reinforcement learning replay mechanism that uses rubric-based prioritization to optimize continual learning through evaluation and adaptive curriculum design">
    <meta name="robots" content="index,follow,max-image-preview:large">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="tokenbender">
    <meta property="og:title" content="infinite - a rubric driven prioritized replay to maximise continual learning">
    <meta property="og:description" content="a reinforcement learning replay mechanism that uses rubric-based prioritization to optimize continual learning through evaluation and adaptive curriculum design">
    <meta property="og:url" content="https://tokenbender.com/posts/infinite-a-rubric-driven-prioritized-replay-to-maximise-continual-learning/">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="infinite - a rubric driven prioritized replay to maximise continual learning">
    <meta name="twitter:description" content="a reinforcement learning replay mechanism that uses rubric-based prioritization to optimize continual learning through evaluation and adaptive curriculum design">
    <link rel="canonical" href="https://tokenbender.com/posts/infinite-a-rubric-driven-prioritized-replay-to-maximise-continual-learning/">
    <script>(function () {
    const storageKey = 'tokenbender-theme';
    const root = document.documentElement;
    let storedTheme = null;
    try {
        storedTheme = window.localStorage.getItem(storageKey);
    } catch (error) {}

    if (storedTheme === 'light' || storedTheme === 'dark') {
        root.setAttribute('data-theme', storedTheme);
        return;
    }

    const prefersLight = window.matchMedia && window.matchMedia('(prefers-color-scheme: light)').matches;
    root.setAttribute('data-theme', prefersLight ? 'light' : 'dark');
})();</script>
    <link rel="stylesheet" href="/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css">
    <script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"infinite - a rubric driven prioritized replay to maximise continual learning","description":"a reinforcement learning replay mechanism that uses rubric-based prioritization to optimize continual learning through evaluation and adaptive curriculum design","author":{"@type":"Person","name":"tokenbender"},"publisher":{"@type":"Person","name":"tokenbender"},"url":"https://tokenbender.com/posts/infinite-a-rubric-driven-prioritized-replay-to-maximise-continual-learning/","mainEntityOfPage":{"@type":"WebPage","@id":"https://tokenbender.com/posts/infinite-a-rubric-driven-prioritized-replay-to-maximise-continual-learning/"},"datePublished":"2025-08-20T00:00:00.000Z","dateModified":"2025-08-20T00:00:00.000Z"}</script>
</head>
<body>
    <header>
        <nav>
            <div class="nav-container">
                <a href="/index.html" class="logo">tokenbender</a>
                <div class="nav-links">
                    <a href="/posts/">archive</a>
                    <a href="https://tokenbender.com/posts/infinite-a-rubric-driven-prioritized-replay-to-maximise-continual-learning/">post</a>
                    <button type="button" class="theme-toggle" data-theme-toggle aria-label="switch theme">light</button>
                </div>
            </div>
        </nav>
    </header>

    <main class="container">
        <article class="post-content" id="post-content">
            <div class="post-date">August 20, 2025</div>
            <h1>infinite - a rubric driven prioritized replay to maximise continual learning</h1>
            <blockquote>
<p>&quot;an infinite game is played for the purpose of continuing the play. not for winning or achieving a specific end.&quot; — James P. Carse</p>
</blockquote>
<h2>abstract</h2>
<p>continual learning systems face a fundamental challenge: how to efficiently retain and build upon previously learned knowledge while adapting to new information. traditional training methods often suffer from catastrophic forgetting and inefficient resource utilization.</p>
<p>infinite introduces a <strong>rubric-driven prioritized replay mechanism</strong> that transforms how continual learning systems select, prioritize, and replay experiences. by implementing a diverse and adaptive evaluation framework, infinite aims to ensure that the most educationally valuable experiences are replayed with optimal frequency.</p>
<p><strong>key innovations:</strong></p>
<ul>
<li><strong>curriculum-based domain selection</strong>: dynamically prioritizes training domains based on performance bands (low/medium/high) and staleness metrics</li>
<li><strong>on-policy training with fresh rollouts</strong>: maintains policy freshness without storing old trajectories, using distributed state tracking across domains</li>
<li><strong>contamination detection</strong>: pre-training validation ensures evaluation data hasn&#39;t leaked into training sets</li>
<li><strong>upgrade mode</strong>: enhances post-trained models with new capabilities while preserving prior skills via KL anchoring</li>
<li><strong>mixed/single batch alternation</strong>: alternates between focused single-domain and cross-domain mixed batches for optimal generalization</li>
</ul>
<p>this approach addresses both (1) minimizing forgetfulness across multiple domains over long horizons, and (2) upgrading post-trained models when original training data is unavailable.</p>
<hr>
<h2>understanding infinite replay</h2>
<p>imagine you&#39;re training an AI system to master multiple domains. these domains are not necessarily related to each other. they may also vary in complexity. you want to have a single base model that can learn from all of these domains. but also it should be able to learn from new domains as they come in. currently we don&#39;t have anything that tackles this effectively. my question is: how do we do this with what we already have?</p>
<p>there are many approaches to this problem. most of them being architecture level changes. i want to explore the possibility of doing this with the assumption that it is already possible given the right training methodology.</p>
<h2>the intuition</h2>
<p>the idea borrows from spaced repetition mechanisms - replay the most important experiences with highest frequency while maintaining minimal coverage of stable areas.</p>
<p><strong>concrete mechanisms:</strong></p>
<p><strong>performance band assignment:</strong></p>
<ul>
<li>convert rubric grades (1-4 scale) to pass/fail indicators (pass ≥ 3)</li>
<li>track exponential moving average (EMA) of pass rates per domain</li>
<li>assign performance bands: low (&lt;0.4), medium (0.4-0.8), high (&gt;0.8)</li>
</ul>
<p><strong>adaptive scheduling priorities:</strong></p>
<ul>
<li>low performers: 60% of training capacity (frequent practice)</li>
<li>medium performers: 30% capacity (regular practice)  </li>
<li>high performers: 10% capacity (occasional refresh)</li>
<li>staleness boost: domains not seen recently get priority increase</li>
<li>uncertainty factor: high variance in recent grades indicates exploration value</li>
</ul>
<p><strong>anti-forgetting feedback loop:</strong></p>
<pre><code>domain performance drops → low band assignment → increased sampling priority → 
more training → performance recovery → higher band → reduced sampling
</code></pre>
<p><strong>distributed state tracking:</strong>
each domain maintains: <code>acc_ema</code>, <code>performance_band</code>, <code>last_seen_step</code>, <code>grade_uncertainty</code></p>
<p>this creates a self-regulating system where struggling domains automatically receive more attention while stable domains are maintained with minimal overhead.</p>
<h2>visual flow</h2>
<pre><code>INFINITE: Rubric-Driven Prioritized Replay for Continual Learning
═══════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────┐
│              INPUT DOMAINS              │
│                                         │
│  ┌─────────────┐ ┌─────────────┐        │
│  │   Domain 1  │ │   Domain 2  │        │
│  │   (Math)    │ │  (Language) │        │
│  └─────────────┘ └─────────────┘        │
│                                         │
│  ┌─────────────┐ ┌─────────────┐        │
│  │   Domain 3  │ │   Domain N  │        │
│  │  (Science)  │ │  (New Task) │        │
│  └─────────────┘ └─────────────┘        │
└─────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────┐
│         RUBRIC EVALUATION MODULE       │
│                                         │
│  ├─ Performance Assessment              │
│  ├─ Cross-Domain Scoring                │
│  └─ Task-Specific Metrics              │
└─────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────┐
│       SCORE DRIFT DETECTION MODULE     │
│                                         │
│  ├─ Value Function Tracking             │
│  ├─ Performance Change Analysis         │
│  └─ Forgetting Detection Algorithm      │
└─────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────┐
│      PRIORITIZED REPLAY SCHEDULER      │
│                                         │
│  ├─ Adaptive Curriculum Generation      │
│  ├─ Spaced Repetition Algorithm         │
│  └─ Priority Queue Management          │
└─────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────┐
│           REPLAY EXECUTION             │
│                                         │
│  ├─ High Priority: Slipping Domains     │
│  ├─ Medium Priority: New Learning       │
│  └─ Low Priority: Stable Domains       │
└─────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────┐
│            CONTINUAL LEARNING           │
│             OBJECTIVES                  │
│                                         │
│  ✓ Knowledge Retention                  │
│  ✓ New Domain Acquisition               │
│  ✓ Catastrophic Forgetting Prevention   │
│  ✓ Adaptive Learning Rate               │
└─────────────────────────────────────────┘

PROCESS FLOW: Domains → Evaluation → Detection → Scheduling → Execution → Learning
</code></pre>
<h2>contamination detection and data integrity</h2>
<p>before any training begins, infinite implements comprehensive contamination detection to ensure evaluation data hasn&#39;t leaked into training sets:</p>
<p><strong>pre-training validation protocol:</strong></p>
<ul>
<li>sample representative subsets from each training domain</li>
<li>compute semantic similarity (cosine similarity of embeddings) between training and evaluation prompts</li>
<li>flag matches exceeding configurable threshold (default: 0.95 cosine similarity)</li>
<li>generate detailed contamination audit report with exact matches and near-duplicates</li>
<li>either automatically remove contaminated samples or halt with error if contamination exceeds tolerance</li>
<li>persist contamination logs for reproducibility and compliance</li>
</ul>
<p><strong>why this matters:</strong> contaminated evaluation data leads to inflated performance metrics and false confidence in model capabilities. this validation step ensures legitimate measurement of continual learning progress.</p>
<h2>implementation architecture</h2>
<p><strong>core training loop (every step k):</strong></p>
<ol>
<li><p><strong>domain state tracking</strong></p>
<ul>
<li>maintain domain statistics (performance, staleness, uncertainty)</li>
<li>synchronize across training nodes when distributed</li>
</ul>
</li>
<li><p><strong>domain health assessment</strong></p>
<ul>
<li>calculate performance bands from acc_ema thresholds</li>
<li>compute staleness (steps since domain last trained)</li>
<li>measure uncertainty from recent grade variance</li>
</ul>
</li>
<li><p><strong>batch composition strategy</strong></p>
<ul>
<li>every 10th step: single-domain batch (focused learning)</li>
<li>other steps: mixed-domain batch (cross-domain transfer)</li>
<li>research suggests 12% better generalization from alternation</li>
</ul>
</li>
<li><p><strong>priority-driven domain selection</strong></p>
<pre><code class="language-python">priority = band_weight + staleness_factor + uncertainty_factor + base_weight
domain_shares = softmax(priorities + anti_starvation_epsilon)
</code></pre>
</li>
<li><p><strong>rollout execution and grading</strong></p>
<ul>
<li>generate model responses for selected prompts</li>
<li>evaluate using domain-specific rubrics (1-4 scale)</li>
<li>update acc_ema with pass/fail indicators</li>
</ul>
</li>
<li><p><strong>GRPO gradient updates</strong> with KL regularization</p>
</li>
</ol>
<p><strong>key components to build:</strong></p>
<ul>
<li><strong>InfiniteGRPOTrainer</strong>: extends base GRPO with curriculum scheduling</li>
<li><strong>TriageStateManager</strong>: persistent distributed state storage  </li>
<li><strong>ContaminationDetector</strong>: pre-training validation (0.95 cosine similarity threshold)</li>
<li><strong>TriageSampler</strong>: priority calculation and batch assembly</li>
<li><strong>RubricEvaluator</strong>: 4-point grading framework</li>
</ul>
<p><strong>domain state structure:</strong></p>
<pre><code class="language-python">class TriageState:
    acc_ema: float           # exponential moving average of pass rates
    performance_band: str    # &quot;low&quot;, &quot;medium&quot;, &quot;high&quot; 
    last_seen_step: int      # staleness tracking
    grade_uncertainty: float # variance in recent rubric scores
</code></pre>
<h2>upgrade mode: enhancing post-trained models</h2>
<p>upgrade mode addresses a critical real-world scenario: you have a well-trained model M0, but original training data is unavailable. you want to add new capabilities without losing existing skills.</p>
<p><strong>initialization workflow:</strong></p>
<ol>
<li><strong>contamination check</strong>: validate new training data doesn&#39;t overlap with evaluation suites</li>
<li><strong>baseline establishment</strong>: evaluate frozen model M0 on all domain evaluation suites</li>
<li><strong>state initialization</strong>: set initial acc_ema values for each domain based on M0&#39;s performance</li>
<li><strong>anchor setup</strong>: configure KL divergence penalties toward M0 to prevent forgetting</li>
</ol>
<p><strong>training modifications:</strong></p>
<ul>
<li><strong>conservative scheduling</strong>: allocate 70% batch capacity to new domains, 30% to prior domain maintenance</li>
<li><strong>stronger KL regularization</strong>: apply higher penalties (coefficient ≥0.1) to maintain similarity to M0</li>
<li><strong>anti-starvation guarantees</strong>: ensure prior domains get minimum sampling even when performing well</li>
<li><strong>gradual capability transfer</strong>: start with lower learning rates on new domains</li>
</ul>
<p><strong>safety mechanisms:</strong></p>
<ul>
<li><strong>regression monitoring</strong>: track performance on prior domains at every evaluation</li>
<li><strong>multi-tier alerts</strong>: escalating responses when prior domain performance drops</li>
<li><strong>automatic rollback</strong>: return to previous checkpoint if regression exceeds thresholds</li>
<li><strong>human-in-the-loop gating</strong>: require manual review after repeated safety violations</li>
</ul>
<h2>evaluation metrics and success criteria</h2>
<p><strong>forgetting and retention tracking:</strong></p>
<ul>
<li><strong>backward transfer (BWT)</strong>: performance change on earlier domains after learning new ones</li>
<li><strong>forward transfer (FWT)</strong>: zero-shot performance gains on unseen domains  </li>
<li><strong>average accuracy (ACC)</strong>: macro-average across all domains over time</li>
<li><strong>area under retention curve (AURC)</strong>: long-term stability per domain</li>
<li><strong>time-to-decay</strong>: steps before performance degrades without practice</li>
</ul>
<p><strong>continual learning benchmarks:</strong></p>
<pre><code class="language-python"># stability curves showing per-domain pass-rate EMA over training steps
stability_curve = acc_ema_per_domain_over_time

# compare against baselines
baseline_grpo = standard_grpo_without_scheduling
infinite_a1 = curriculum_domain_selection_only  
infinite_a2 = add_staleness_priority_boosting
infinite_a3 = add_uncertainty_factors
infinite_full = mixed_single_batch_alternation
</code></pre>
<p><strong>success criteria:</strong></p>
<ul>
<li>curriculum scheduling improves AURC by ≥25% over baseline GRPO</li>
<li>mixed/single alternation shows measurable generalization benefit</li>
<li>upgrade mode: new domain improvement ≥5 points, prior domain drop ≤1 point</li>
<li>contamination detection catches known overlaps with 95%+ accuracy</li>
</ul>
<p><strong>safety gating policy for upgrade mode:</strong></p>
<ul>
<li>first alert: increase domain bucket weight to boost sampling</li>
<li>second alert: strengthen KL penalty toward baseline model M0  </li>
<li>third alert: reduce new domain sampling temporarily</li>
<li>final gate: halt training and require human review after H failed evaluations</li>
</ul>
<h2>planning the details</h2>
<p>we need to plan the details of the implementation.</p>
<ul>
<li>choice of base model?</li>
<li>which domains to use?</li>
<li>detecting catastrophic forgetting in standard RL training for the base model?</li>
<li>what to measure for each domain?</li>
<li>expected challenges with reward hacking?</li>
<li>known works that tackle this or something similar?</li>
<li>rough timeline?</li>
<li>what people with various backgrounds can contribute?</li>
</ul>
<h2>division by contribution areas</h2>
<p>broadly there are six areas of contribution where there are lots of things to be done: </p>
<ul>
<li>contamination check scripts - to test the base/instruct model on the domains we pick </li>
<li>collecting small datasets, evals, RL env for math/code/creative language tasks</li>
<li>contributing to code based on already decided algorithms (scheduling the replay, how to weight the domains, any other policy gradient design decisions)</li>
<li>contributing to improving the algorithms based on some identified disadvantage</li>
<li>compute/running experiments</li>
<li>miscellaneous (any software level, uncategorised feedback/improvement)</li>
</ul>
<p>this is a work in progress.
all updates will be posted here.</p>
<p><strong>join the discussion:</strong></p>
<ul>
<li><a href="https://discord.gg/YaYfPu4ZT4">e/Xperiments discord server</a></li>
<li><strong>github repo</strong>: <a href="https://github.com/tokenbender/infinite">https://github.com/tokenbender/infinite</a></li>
</ul>
<p>reach out if you think it is cool and can contribute in any way - collaboration, compute or sponsorship.</p>

        </article>
    </main>

    <footer>
        <p>for updates and random thoughts, follow <a href="https://x.com/tokenbender" target="_blank" rel="noopener">@tokenbender</a>.</p>
    </footer>

    <script>(function () {
    const storageKey = 'tokenbender-theme';
    const root = document.documentElement;
    const toggle = document.querySelector('[data-theme-toggle]');

    const getTheme = () => root.getAttribute('data-theme') === 'light' ? 'light' : 'dark';

    const renderToggle = (activeTheme) => {
        if (!toggle) {
            return;
        }

        const targetTheme = activeTheme === 'light' ? 'dark' : 'light';
        toggle.textContent = targetTheme;
        toggle.setAttribute('aria-label', `switch to ${targetTheme} theme`);
        toggle.setAttribute('aria-pressed', activeTheme === 'light' ? 'true' : 'false');
    };

    const setTheme = (theme, persist) => {
        root.setAttribute('data-theme', theme);
        renderToggle(theme);
        if (persist) {
            try {
                window.localStorage.setItem(storageKey, theme);
            } catch (error) {}
        }
    };

    setTheme(getTheme(), false);

    if (toggle) {
        toggle.addEventListener('click', () => {
            const nextTheme = getTheme() === 'light' ? 'dark' : 'light';
            setTheme(nextTheme, true);
        });
    }

    if (window.matchMedia) {
        const media = window.matchMedia('(prefers-color-scheme: light)');
        const onChange = (event) => {
            let storedTheme = null;
            try {
                storedTheme = window.localStorage.getItem(storageKey);
            } catch (error) {}

            if (storedTheme === 'light' || storedTheme === 'dark') {
                return;
            }

            setTheme(event.matches ? 'light' : 'dark', false);
        };

        if (typeof media.addEventListener === 'function') {
            media.addEventListener('change', onChange);
        } else if (typeof media.addListener === "function") {
            media.addListener(onChange);
        }
    }
})();</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/contrib/auto-render.min.js"></script>
    <script>
        if (window.renderMathInElement) {
            window.renderMathInElement(document.getElementById('post-content'), {
                delimiters: [
                    { left: '$$', right: '$$', display: true },
                    { left: '$', right: '$', display: false },
                    { left: '\\(', right: '\\)', display: false },
                    { left: '\\[', right: '\\]', display: true }
                ],
                throwOnError: false,
                trust: false
            });
        }

        if (window.Prism) {
            window.Prism.highlightAll();
        }
    </script>
</body>
</html>
